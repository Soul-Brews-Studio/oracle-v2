# Session Retrospective

**Session Date**: 2026-01-26
**Start Time**: 11:30 GMT+7
**End Time**: 12:51 GMT+7
**Duration**: ~80 minutes
**Primary Focus**: Gesture controls and laser pointer for 3D graph hand tracking
**Session Type**: Feature Development
**Current Issue**: N/A (continuation of hand tracking work)
**Last PR**: N/A (working on main)

## Session Summary

Extended the hand tracking feature in the Oracle 3D Knowledge Graph with gesture detection (fist, open, point, peace), gesture-based zoom controls, and a red laser pointer with trailing tail for precise node selection. Implemented screen-space hover detection that works at any zoom level, and added extensive position smoothing with dead zones for stable pointing.

## Timeline

- 11:30 - Continued session from lightning effects, focused on hand tracking improvements
- 11:45 - Added gesture detection to useHandTracking hook (fist, open, point, peace)
- 12:00 - Implemented gesture-based zoom: fist = zoom in, release = back to original
- 12:10 - Added red laser pointer with 12-dot trailing tail when pointing
- 12:20 - Implemented screen-space hover detection (works at any zoom)
- 12:25 - Made laser turn green when hovering a node (visual feedback)
- 12:35 - Fixed tooltip positioning for laser hover
- 12:40 - Added position smoothing to reduce hand shake
- 12:45 - Enhanced pointing stability with heavy smoothing (0.02) and dead zone (2%)
- 12:50 - Reviewed mission-03-gesture-control for reference patterns
- 12:51 - Committed gesture controls feature

## Technical Details

### Files Modified
```
frontend/src/hooks/useHandTracking.ts
frontend/src/pages/Graph.tsx
```

### Key Code Changes

- **useHandTracking.ts**:
  - Added gesture detection function analyzing MediaPipe landmarks
  - Finger extended = fingertip Y < MCP (knuckle) Y
  - Returns: 'fist', 'open', 'point', 'peace', or null
  - Added exponential moving average smoothing
  - Heavy smoothing (0.02) + dead zone (2%) when pointing

- **Graph.tsx**:
  - Gesture-based zoom: fist zooms in, release returns to original
  - Point gesture holds zoom level for precise selection
  - Red laser pointer with trailing tail (12 dots, fading opacity)
  - Laser turns green when hovering node
  - Screen-space hover detection (project 3D â†’ 2D, find closest within 40px)
  - Tooltip positioned using same absolute % as laser dot

### Architecture Decisions

- **Screen-space hover detection**: Raycasting failed at close zoom because the 3D geometry doesn't change size. Switched to projecting nodes to 2D and checking pixel distance - matches what user visually sees.

- **Heavy smoothing when pointing**: Normal smoothing (0.3) responsive for navigation, but pointing needs stability. Used 0.02 smoothing + dead zone to nearly freeze the pointer.

- **Laser tail as individual divs**: SVG polyline had scaling issues with viewBox. Individual div dots with radial gradients render correctly at any size.

## AI Diary

This session was a fascinating deep dive into the nuances of hand tracking user experience. I started with what seemed like a straightforward task - add gesture detection - but quickly discovered that the real challenge wasn't detection, it was usability.

The gesture detection itself was elegant: MediaPipe gives 21 landmarks, and determining if a finger is extended is simply comparing the fingertip Y to the knuckle (MCP) Y position. Fist = no fingers extended, point = only index extended. The logic was clean and worked immediately.

But then came the real challenges. First, hover detection broke when zoomed in close - the user showed me a screenshot where the laser was right next to a node but not detecting it. I initially tried raycaster adjustments, but realized the fundamental issue: the 3D geometry doesn't change size when you zoom, only its visual representation does. The solution was elegant - project all nodes to 2D screen coordinates and find the closest one within a pixel threshold. This matches exactly what the user sees.

The hand shake issue taught me about the importance of context-sensitive smoothing. When navigating, you want responsiveness (smoothing factor 0.3). But when pointing at a node to select it, you need stability - the user's hand naturally shakes, and tiny movements shouldn't move the pointer. I ended up with aggressive smoothing (0.02) plus a dead zone that ignores movements smaller than 2% when pointing.

I also appreciated seeing the mission-03-gesture-control reference implementation. It used a similar EMA approach for palm size smoothing (0.2 factor) and had the same philosophy of fist = freeze/stop. Seeing working patterns in another codebase helped validate my approach.

## What Went Well

- Gesture detection worked on first implementation
- Screen-space hover detection solved zoom-distance problem elegantly
- Laser visual feedback (green = hovering) makes debugging intuitive
- Heavy smoothing + dead zone made pointing rock-steady
- Found relevant reference in mission-03 codebase

## What Could Improve

- Should have started with screen-space detection instead of raycasting
- Tooltip positioning went through multiple iterations before getting right
- Could add node selection on point gesture (click without clicking)

## Blockers & Resolutions

- **Blocker**: Hover detection failed when zoomed in close
  **Resolution**: Switched from 3D raycasting to 2D screen-space distance checking

- **Blocker**: Tooltip appeared at wrong position for laser hover
  **Resolution**: Used same absolute positioning as laser dot (position: absolute with % values)

- **Blocker**: Hand shake made pointing imprecise
  **Resolution**: Heavy smoothing (0.02) + dead zone (2%) when pointing

## Honest Feedback

This session demonstrated both the power and the iteration required for hand tracking interfaces. The user's feedback was invaluable - "when close can't match" with a screenshot immediately clarified the raycasting limitation. "my hand is shake" and "fixed position is better! if finger point" led directly to the dead zone solution.

The mission-03 reference request was helpful - seeing another working implementation validates approaches and reveals patterns. The EMA smoothing was identical, which gave confidence the technique is sound.

I'm impressed by how natural the gesture controls feel once the smoothing is right. Fist to zoom in, release to zoom out, point to select - it maps intuitively to physical gestures. The laser trail adds visual flair while the green-on-hover provides essential feedback.

### Friction Points (3 required)

1. **Raycasting assumptions**: I assumed 3D raycasting would "just work" at any zoom level. Had to learn that screen-space calculations are more reliable for user interactions. Impact: Wasted time debugging raycaster thresholds before realizing the approach was wrong.

2. **Tooltip positioning complexity**: Fixed vs absolute positioning, container bounds vs viewport coordinates - took multiple iterations. Impact: Should have matched the laser dot's positioning approach from the start.

3. **Smoothing factor tuning**: Started with 0.3 for everything, then 0.08 for pointing, finally 0.02 + dead zone. Impact: Needed user feedback to find right values - no way to know "right" smoothing without testing.

## Lessons Learned

- **Pattern**: Screen-space calculations for UI interactions - project 3D to 2D and work in pixels. Why: Matches what users see, independent of camera distance.

- **Pattern**: Context-sensitive smoothing - different modes need different responsiveness. Navigation = responsive (0.3), precision = stable (0.02 + dead zone).

- **Discovery**: MediaPipe gesture detection is simple geometry - fingertip Y vs knuckle Y determines if finger is extended.

## Next Steps

- [ ] Commit the smoothing improvements
- [ ] Consider adding node selection on point gesture (click without clicking)
- [ ] Test with different lighting/background conditions
- [ ] Create PR with all hand tracking improvements

## Metrics

- **Commits**: 1 (gesture controls + laser pointer)
- **Files changed**: 2 (useHandTracking.ts, Graph.tsx)
- **Lines added**: ~150
- **Lines removed**: ~30
- **Tests**: Manual testing passed

## Retrospective Validation Checklist

- [x] AI Diary section has detailed narrative (not placeholder)
- [x] Honest Feedback section has frank assessment (not placeholder)
- [x] Timeline includes actual times and events
- [x] 3 Friction Points documented
- [x] Lessons Learned has actionable insights
- [x] Next Steps are specific and achievable
